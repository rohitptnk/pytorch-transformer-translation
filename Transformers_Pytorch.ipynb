{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitptnk/pytorch-transformer-translation/blob/main/Transformers_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce7b15c7",
      "metadata": {
        "id": "ce7b15c7"
      },
      "source": [
        "# Transformer in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SjgkjdlQVKw",
        "outputId": "900a69d5-5724-4f3a-9a28-4705513033f7"
      },
      "id": "4SjgkjdlQVKw",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 17 09:21:20 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0             33W /   70W |    1772MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fef5ce7",
      "metadata": {
        "id": "4fef5ce7"
      },
      "source": [
        "# config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "5dedc833",
      "metadata": {
        "id": "5dedc833"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 4,\n",
        "        \"num_epochs\": 10,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 350,\n",
        "        \"d_model\": 128,\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"it\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": None,\n",
        "        \"tokenizer_file\":\"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/model\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07b2ecb",
      "metadata": {
        "id": "d07b2ecb"
      },
      "source": [
        "# model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b8e18c",
      "metadata": {
        "id": "45b8e18c"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "37021823",
      "metadata": {
        "id": "37021823"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1dc423b",
      "metadata": {
        "id": "c1dc423b"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "9b1c6118",
      "metadata": {
        "id": "9b1c6118"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float ) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Vector of shape (Seq_len, 1) for position\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # Apply sin and cos\n",
        "        pe[:, 0::2] = torch.sin(position*div_term)\n",
        "        pe[:, 1::2] = torch.cos(position*div_term)\n",
        "\n",
        "        # print(\"\\n pe shape before\", pe.shape)\n",
        "        # Add another dimension for batching\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # print(\"\\npe shape after:\", pe.shape)\n",
        "\n",
        "        # Save as a non-parameter tensor\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3755b2a6",
      "metadata": {
        "id": "3755b2a6"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "6a732606",
      "metadata": {
        "id": "6a732606"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, eps: float = 10**-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim=True)\n",
        "        std = x.std(dim = -1, keepdim=True)\n",
        "        return self.alpha * (x - mean)/(std + self.eps) + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795bdd5b",
      "metadata": {
        "id": "795bdd5b"
      },
      "source": [
        "## Feed Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "29d2d9bb",
      "metadata": {
        "id": "29d2d9bb"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, droupout: float):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 and B1\n",
        "        self.dropout = nn.Dropout(droupout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 and B2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (Batch, seq_length, d_model) --> (Batch, seq_length, d_ff) --> (Batch, seq_length, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f43c7e",
      "metadata": {
        "id": "b2f43c7e"
      },
      "source": [
        "## Multi Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b0030cd8",
      "metadata": {
        "id": "b0030cd8"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float): # h indicates the number of heads\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model, d_model) #Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model) #Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model) #Wv\n",
        "\n",
        "        self.w_o = nn.Linear(d_model, d_model) #Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "\n",
        "        # query @ key = (batch, h, seq_len, d_k) @ (batch, h, d_k, seq_len) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1))/math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim = -1) # (batch, h, seq_len, seq_len)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        # att_score @ val = (batch, h, seq_len, seq_len) @ (batch, h, seq_len, d_v) --> (batch, h, seq_len, d_v)\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "        # Splitting the d_model dimension into h pieces of size d_k for h attention heads\n",
        "        # (batch, seq_len, d_model) -->  (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k) ((transposing for easier multiplication))\n",
        "        query = query.view(query.shape[0],  query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "        key = key.view(key.shape[0],  key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "        value = value.view(value.shape[0],  value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        x, attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # (batch, h, seq_len, d_v) --> (batch, seq_len, h, d_v) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acd9c8df",
      "metadata": {
        "id": "acd9c8df"
      },
      "source": [
        "## Residual Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d4883f09",
      "metadata": {
        "id": "d4883f09"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x))) # We apply the sublayer(MHA) after the normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7fccf1",
      "metadata": {
        "id": "bd7fccf1"
      },
      "source": [
        "## Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "2e32ee99",
      "metadata": {
        "id": "2e32ee99"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.dropout = dropout\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98022a06",
      "metadata": {
        "id": "98022a06"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "3433767c",
      "metadata": {
        "id": "3433767c"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78bd760",
      "metadata": {
        "id": "b78bd760"
      },
      "source": [
        "## Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "aafb2b55",
      "metadata": {
        "id": "aafb2b55"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.dropout = dropout\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc23f10a",
      "metadata": {
        "id": "bc23f10a"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ebcd19a2",
      "metadata": {
        "id": "ebcd19a2"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72afae70",
      "metadata": {
        "id": "72afae70"
      },
      "source": [
        "## Linear (Projection) Layer + Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "6c6ea6dd",
      "metadata": {
        "id": "6c6ea6dd"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return torch.log_softmax(self.proj(x), dim = -1) #log softmax for numerical stability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919c89d8",
      "metadata": {
        "id": "919c89d8"
      },
      "source": [
        "## Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "952b4103",
      "metadata": {
        "id": "952b4103"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "        super().__init__()\n",
        "        self.src_embed = src_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.encoder = encoder\n",
        "\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.decoder = decoder\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d2b9cb",
      "metadata": {
        "id": "20d2b9cb"
      },
      "source": [
        "## Combining it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "28db2947",
      "metadata": {
        "id": "28db2947"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 128, N: int = 2, h: int=2, dropout: float = 0.1, d_ff: int = 1024): # N=num_layers, h=num_heads\n",
        "    # Create embedding layer\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create encoder and decoder\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062b4eba",
      "metadata": {
        "id": "062b4eba"
      },
      "source": [
        "# dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "4b78ea55",
      "metadata": {
        "id": "4b78ea55"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_src.token_to_id('[PAD]')], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        # print(\"length of enc_tokens:\", len(enc_input_tokens))\n",
        "\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "        # print(\"length of dec:\", len(dec_input_tokens))\n",
        "        # print(\"seq len:\", self.seq_len)\n",
        "\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        # print(\"enc pad:\", enc_num_padding_tokens)\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "        # print(\"dec pad:\", dec_num_padding_tokens)\n",
        "\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError('Sentence is too long.')\n",
        "\n",
        "        # Add SOS and EOS tokens and padding to the source text\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add SOS to decoder input\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add EOS to the label (what we expect as output from the decoder)\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        assert encoder_input.shape[0] == self.seq_len\n",
        "        assert decoder_input.shape[0] == self.seq_len\n",
        "        assert label.shape[0] == self.seq_len\n",
        "\n",
        "        return {\n",
        "            'encoder_input': encoder_input, # (seq_len,)\n",
        "            'decoder_input': decoder_input, # (seq_len,)\n",
        "            'encoder_mask' : (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            'decoder_mask' : (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, 1, seq_len) & (1, seq_len, seq_len)\n",
        "            'label': label, # (seq_len,)\n",
        "            'src_text': src_text,\n",
        "            'tgt_text': tgt_text\n",
        "        }\n",
        "\n",
        "def causal_mask(size: int):\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3629d163",
      "metadata": {
        "id": "3629d163"
      },
      "source": [
        "# train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f334e2",
      "metadata": {
        "id": "89f334e2"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "a7f5c21b",
      "metadata": {
        "id": "a7f5c21b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# from dataset import BilingualDataset, causal_mask\n",
        "# from model import build_transformer\n",
        "# from config import get_weights_file_path, get_config\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "  sos_idx = tokenizer_src.token_to_id('[SOS]')\n",
        "  eos_idx = tokenizer_src.token_to_id('[EOS]')\n",
        "\n",
        "  # precompute the encoder output and reuse it for every token we get from the decoder\n",
        "  encoder_output = model.encode(source, source_mask)\n",
        "  # initialize decoder input with the start of sentence token\n",
        "  decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "  while True:\n",
        "    if decoder_input.size(1) == max_len:\n",
        "      break\n",
        "\n",
        "    # build mask for target (decoder input)\n",
        "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "    # calculate the output\n",
        "    out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "    # get the next token\n",
        "    prob = model.project(out[:, -1])\n",
        "    # select the token with the max probability (greedy search)\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "    if next_word == eos_idx:\n",
        "      break\n",
        "  return decoder_input.squeeze(0)\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
        "  model.eval()\n",
        "  count = 0\n",
        "\n",
        "  # source_texts = []\n",
        "  # expected = []\n",
        "  # predicted = []\n",
        "\n",
        "  # Size of the control window (just use a default vaule)\n",
        "  console_width = 80\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in validation_ds:\n",
        "      count += 1\n",
        "      encoder_input = batch['encoder_input'].to(device)\n",
        "      encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "      assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "      model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "      source_text = batch['src_text'][0]\n",
        "      target_text = batch['tgt_text'][0]\n",
        "      model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "      # source_texts.append(source_text)\n",
        "      # expected.append(target_text)\n",
        "      # predicted.append(model_out_text)\n",
        "\n",
        "      # Print to the console\n",
        "      print_msg('-'*console_width)\n",
        "      print_msg(f'SOURCE: {source_text}')\n",
        "      print_msg(f'TARGET: {target_text}')\n",
        "      print_msg(f'PREDICTED: {model_out_text}')\n",
        "\n",
        "      if count == num_examples:\n",
        "        break\n",
        "\n",
        "    # if writer:\n",
        "    #   # TorchMetrics ChartErrorRate, BLEU, WordErrorRate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens = ['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency = 2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b46864a",
      "metadata": {
        "id": "0b46864a"
      },
      "source": [
        "## Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "96c1dcec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96c1dcec",
        "outputId": "397654cc-a06b-44f5-d75b-226a5d9230ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 00: 100%|██████████| 7275/7275 [06:06<00:00, 19.83it/s, loss=4.501]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: When her gaze now met his kindly blue eyes looking steadily at her, it seemed to her that he saw right through her, and knew all the trouble that was in her.\n",
            "TARGET: E quando il suo sguardo incontrò quei suoi buoni occhi azzurri che la guardavano fissi, le sembrò ch’egli la vedesse da parte a parte e che capisse tutto il tormento che avveniva in lei.\n",
            "PREDICTED: ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ gli l ’ l ’ l ’ l ’ l ’ l ’ gli gli gli l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Inside we were a still greater success.\n",
            "TARGET: Dentro il teatro, il successo fu ancora maggiore.\n",
            "PREDICTED: ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ gli l ’ l ’ l ’ l ’ l ’ l ’ gli gli l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l ’ l\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 01: 100%|██████████| 7275/7275 [05:37<00:00, 21.56it/s, loss=2.902]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Jane, you would not repent marrying me--be certain of that; we _must_ be married.\n",
            "TARGET: Jane, — riprese a voce più alta, — non vi pentirete di avermi sposato.\n",
            "PREDICTED: Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un della Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: When they were still about two hundred paces from the house the wind had already risen, so that at any moment a downpour might be expected.\n",
            "TARGET: Fino a casa c’erano ancora duecento passi, e s’era già alzato il vento e, da un secondo all’altro, ci si poteva aspettare un acquazzone.\n",
            "PREDICTED: Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un Un\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 02: 100%|██████████| 7275/7275 [05:08<00:00, 23.54it/s, loss=2.602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Just as the others began settling round the table and Levin was about to go, the old Prince came in, and having greeted the ladies he turned to Levin.\n",
            "TARGET: Mentre gli altri volevano disporsi attorno al tavolino e Levin cercava di andarsene, entrò il principe e, salutate le signore, si rivolse a Levin.\n",
            "PREDICTED: - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'That coldness, that pretence of feeling!' she said to herself. 'They want to wound me and torture the child, and shall I submit to them?\n",
            "TARGET: “Questa freddezza è la finzione di un sentimento! — diceva a se stessa. — Hanno solo bisogno di offendere me e di tormentare il bambino, e io dovrei sottostare a loro!\n",
            "PREDICTED: - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 03: 100%|██████████| 7275/7275 [05:08<00:00, 23.61it/s, loss=2.168]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"Where is he?\"\n",
            "TARGET: — Dov'è lui?\n",
            "PREDICTED: ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?”\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Don't hurt yourself – it needs practice!'\n",
            "TARGET: — Volete ammazzarvi!\n",
            "PREDICTED: ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?” ?”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 04: 100%|██████████| 7275/7275 [05:07<00:00, 23.66it/s, loss=1.895]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: CHAPTER I.\n",
            "TARGET: CAPITOLO I.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: This straightforward and simple attitude toward her own position pleased Golenishchev.\n",
            "TARGET: Questo atteggiamento leale e semplice dinanzi alla propria situazione, piacque a Golenišcev.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 05: 100%|██████████| 7275/7275 [05:06<00:00, 23.76it/s, loss=1.481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: They speak with the broadest accent of the district. At present, they and I have a difficulty in understanding each other's language.\n",
            "TARGET: Esse parlano col più marcato accento del distretto e per ora duro fatica a capirle, ed esse pure mi capiscono male.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"Very sorry, sir,\" again repeated the landlord: \"but we really haven't got a bed vacant in the whole house.\n",
            "TARGET: — Mi dispiace molto, signore — ripetè di nuovo il padrone; — ma non abbiamo un solo letto vuoto in tutto l’albergo.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 06: 100%|██████████| 7275/7275 [05:07<00:00, 23.62it/s, loss=1.441]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Petritsky was a young lieutenant, not of very aristocratic birth, and not only not wealthy but heavily in debt, tipsy every evening, and often under arrest for amusing or improper escapades, but popular both with his comrades and superiors.\n",
            "TARGET: Petrickij era un giovane tenente non di alto lignaggio, e non solo non ricco, ma affogato nei debiti, sempre brillo verso sera e spesso agli arresti per varie scabrose e ridicole storie, ma amato dai compagni e dai superiori.\n",
            "PREDICTED: Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: From the moment that Anna gave him her love he considered his own right to her indefeasible.\n",
            "TARGET: Sin dal momento in cui Anna si era innamorata di lui, egli riteneva di avere su di lei, egli solo, un suo proprio diritto indiscutibile.\n",
            "PREDICTED: Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed Ed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 07: 100%|██████████| 7275/7275 [05:05<00:00, 23.84it/s, loss=1.727]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: They speak with the broadest accent of the district. At present, they and I have a difficulty in understanding each other's language.\n",
            "TARGET: Esse parlano col più marcato accento del distretto e per ora duro fatica a capirle, ed esse pure mi capiscono male.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Oh, I am certain Jane will agree with me in opinion, when she knows all that I know!\n",
            "TARGET: Oh! sono certo che Jane dividerà la mia opinione, quando saprà tutto quello che so!\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 08: 100%|██████████| 7275/7275 [05:04<00:00, 23.87it/s, loss=1.518]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: My pulse stopped: my heart stood still; my stretched arm was paralysed.\n",
            "TARGET: Il mio polso si fermò, il cuore cessò di battere, il braccio che avevo allungato rimase paralizzato.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: The chief qualities that had won him this general respect in his Office were, first, his extreme leniency, founded on a consciousness of his own defects; secondly, his true Liberalism – not that of which he read in his paper, but that which was in his blood and made him treat all men alike whatever their rank or official position; thirdly and chiefly, his complete indifference to the business he was engaged on, in consequence of which he was never carried away by enthusiasm and never made mistakes.\n",
            "TARGET: Le principali qualità che gli procuravano la stima generale in ufficio consistevano, in primo luogo, in una straordinaria indulgenza verso gli altri, basata sulla coscienza dei propri difetti; in secondo luogo, in un grande liberalismo, non quello di cui leggeva nei giornali, ma quello ch’egli aveva nel sangue e che gli faceva trattare perfettamente allo stesso modo tutte le persone, di qualunque classe o condizione fossero; e in terzo luogo, e questa era la cosa più importante, in un’assoluta indifferenza verso gli affari che trattava, per cui non se ne appassionava mai e non commetteva errori.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 09: 100%|██████████| 7275/7275 [05:01<00:00, 24.10it/s, loss=1.435]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: If you were mad, do you think I should hate you?\"\n",
            "TARGET: Credete che, se voi foste pazza, vi odierei?\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Whosoever shall smite thee on thy right cheek, turn to him the other also... and if any man will take away thy coat, let him have thy cloak also,' thought Karenin.\n",
            "TARGET: “E a colui che avrà percosso la tua guancia destra, tendi la sinistra; e a colui che ti avrà tolto il mantello da’ la tunica” diceva a se stesso Aleksej Aleksandrovic.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n"
          ]
        }
      ],
      "source": [
        "def get_ds(config):\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Train-validate split, keep 90% for training and 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model\n",
        "\n",
        "def train_model(config):\n",
        "    # Define the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device {device}')\n",
        "\n",
        "    Path(config['model_folder']).mkdir(parents= True, exist_ok= True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
        "        for batch in batch_iterator:\n",
        "            model.train()\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (batch, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (batch, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)   # (1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)   # (1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the transformer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (batch, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (batch, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (batch, seq_len, tgt_vocab_size)\n",
        "\n",
        "            label = batch['label'].to(device) # (batch, seq_len)\n",
        "\n",
        "            # (batch, seq_len, tgt_vocab_size) --> (batch * seq_len, tgt_vocab_size)\n",
        "            # (batch, seq_len) --> (batch * seq_len)\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # log the loss\n",
        "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings('ignore')\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Only Code"
      ],
      "metadata": {
        "id": "vwIVdn9--vS1"
      },
      "id": "vwIVdn9--vS1"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from config import get_config, get_weights_file_path\n",
        "# from train import get_model, get_ds, run_validation\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\",device)\n",
        "config = get_config()\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "# Load the pretrained weights\n",
        "model_filename = get_weights_file_path(config, f\"09\")\n",
        "state = torch.load(model_filename)\n",
        "model.load_state_dict(state['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZy6KPXETbod",
        "outputId": "05efea6a-3202-45df-f969-98e2c22bb4d3"
      },
      "id": "lZy6KPXETbod",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGGuhpBmALGg",
        "outputId": "38d2dc87-5202-4959-8bc2-583ea5ed3536"
      },
      "id": "RGGuhpBmALGg",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Can't she manage to walk at her age?\n",
            "TARGET: Non sa camminare alla sua età?\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: It remains now to see what ought to be the rules of conduct for a prince towards subject and friends.\n",
            "TARGET: Resta ora a vedere quali debbano essere e' modi e governi di uno principe con sudditi o con li amici.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: There remained only a short-legged woman who was always lying down because she had a bad figure, and who tormented poor unresisting Varenka for not tucking her plaid the right way.\n",
            "TARGET: Era rimasta ormai una donna con una gamba più corta dell’altra che stava a letto perché era fatta male e tormentava la docile Varen’ka perché non ravvolgeva lo scialle così come andava fatto.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Levin sat down to wait till the professor should go, but soon became interested in the subject of their conversation.\n",
            "TARGET: Levin sedette in attesa che il professore se ne andasse, quando improvvisamente prese interesse all’argomento.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: George requested that we would not talk about these things, at all events until he had finished his cold boiled beef without mustard.\n",
            "TARGET: Giorgio ci pregò di non parlare di simili cose; a ogni modo s’era finito il manzo allesso senza la mostarda.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Why did you not write to me?'\n",
            "TARGET: Perché non m’hai scritto?\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'It was overdriven yesterday, Constantine Dmitrich,' he said. 'Why, it was driven hard for ten versts!'\n",
            "TARGET: — Ieri l’avete fiaccato, Konstantin Dmitric — diceva. — E come l’avete spinto per dieci verste fuori di strada!\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'I understand, I understand very well,' said Dolly and her head dropped.\n",
            "TARGET: — Capisco, capisco bene — disse Dolly, e abbassò il capo.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: And I add this part here, to hint to whoever shall read it, that whenever they come to a true sense of things, they will find deliverance from sin a much greater blessing than deliverance from affliction.\n",
            "TARGET: E aggiungo questo episodio alla mia storia per indicare a chiunque la leggera che, ogni qual volta l’uomo arrivi a scoprire il vero senso delle cose, ravviserà nella liberazione dalla colpa una beatitudine infinitamente maggiore dell’essere liberato da qualsivoglia cordoglio.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: You are my little friend, are you not?\"\n",
            "TARGET: Siete la mia piccola amica, non è vero?\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "gfe7oapQAv1C"
      },
      "id": "gfe7oapQAv1C"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}