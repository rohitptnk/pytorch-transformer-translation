{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitptnk/pytorch-transformer-translation/blob/main/Transformers_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce7b15c7",
      "metadata": {
        "id": "ce7b15c7"
      },
      "source": [
        "# Transformer in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SjgkjdlQVKw",
        "outputId": "e7a7691d-24f7-4021-e21b-db13f78a158d"
      },
      "id": "4SjgkjdlQVKw",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 17 11:15:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0             29W /   70W |     310MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rohitptnk/pytorch-transformer-translation.git\n",
        "!cd pytorch-transformer-translation"
      ],
      "metadata": {
        "id": "o_t-KoHbYHGP",
        "outputId": "ef0a2410-7a63-449e-efb7-8f0b36139df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o_t-KoHbYHGP",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pytorch-transformer-translation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fef5ce7",
      "metadata": {
        "id": "4fef5ce7"
      },
      "source": [
        "# config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "5dedc833",
      "metadata": {
        "id": "5dedc833"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 4,\n",
        "        \"num_epochs\": 10,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 350,\n",
        "        \"d_model\": 128,\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"it\",\n",
        "        \"model_folder\": \"pytorch-transformer-translation/weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": None,\n",
        "        \"tokenizer_file\":\"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/model\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07b2ecb",
      "metadata": {
        "id": "d07b2ecb"
      },
      "source": [
        "# model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b8e18c",
      "metadata": {
        "id": "45b8e18c"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "37021823",
      "metadata": {
        "id": "37021823"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1dc423b",
      "metadata": {
        "id": "c1dc423b"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "9b1c6118",
      "metadata": {
        "id": "9b1c6118"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float ) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Vector of shape (Seq_len, 1) for position\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        # Apply sin and cos\n",
        "        pe[:, 0::2] = torch.sin(position*div_term)\n",
        "        pe[:, 1::2] = torch.cos(position*div_term)\n",
        "\n",
        "        # print(\"\\n pe shape before\", pe.shape)\n",
        "        # Add another dimension for batching\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # print(\"\\npe shape after:\", pe.shape)\n",
        "\n",
        "        # Save as a non-parameter tensor\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3755b2a6",
      "metadata": {
        "id": "3755b2a6"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "6a732606",
      "metadata": {
        "id": "6a732606"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, eps: float = 10**-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim=True)\n",
        "        std = x.std(dim = -1, keepdim=True)\n",
        "        return self.alpha * (x - mean)/(std + self.eps) + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795bdd5b",
      "metadata": {
        "id": "795bdd5b"
      },
      "source": [
        "## Feed Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "29d2d9bb",
      "metadata": {
        "id": "29d2d9bb"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, droupout: float):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 and B1\n",
        "        self.dropout = nn.Dropout(droupout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 and B2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (Batch, seq_length, d_model) --> (Batch, seq_length, d_ff) --> (Batch, seq_length, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f43c7e",
      "metadata": {
        "id": "b2f43c7e"
      },
      "source": [
        "## Multi Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "b0030cd8",
      "metadata": {
        "id": "b0030cd8"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float): # h indicates the number of heads\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model, d_model) #Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model) #Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model) #Wv\n",
        "\n",
        "        self.w_o = nn.Linear(d_model, d_model) #Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "\n",
        "        # query @ key = (batch, h, seq_len, d_k) @ (batch, h, d_k, seq_len) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1))/math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim = -1) # (batch, h, seq_len, seq_len)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        # att_score @ val = (batch, h, seq_len, seq_len) @ (batch, h, seq_len, d_v) --> (batch, h, seq_len, d_v)\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "        # Splitting the d_model dimension into h pieces of size d_k for h attention heads\n",
        "        # (batch, seq_len, d_model) -->  (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k) ((transposing for easier multiplication))\n",
        "        query = query.view(query.shape[0],  query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "        key = key.view(key.shape[0],  key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "        value = value.view(value.shape[0],  value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        x, attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # (batch, h, seq_len, d_v) --> (batch, seq_len, h, d_v) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acd9c8df",
      "metadata": {
        "id": "acd9c8df"
      },
      "source": [
        "## Residual Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "d4883f09",
      "metadata": {
        "id": "d4883f09"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x))) # We apply the sublayer(MHA) after the normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7fccf1",
      "metadata": {
        "id": "bd7fccf1"
      },
      "source": [
        "## Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "2e32ee99",
      "metadata": {
        "id": "2e32ee99"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.dropout = dropout\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98022a06",
      "metadata": {
        "id": "98022a06"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "3433767c",
      "metadata": {
        "id": "3433767c"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78bd760",
      "metadata": {
        "id": "b78bd760"
      },
      "source": [
        "## Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "aafb2b55",
      "metadata": {
        "id": "aafb2b55"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.dropout = dropout\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc23f10a",
      "metadata": {
        "id": "bc23f10a"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "ebcd19a2",
      "metadata": {
        "id": "ebcd19a2"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72afae70",
      "metadata": {
        "id": "72afae70"
      },
      "source": [
        "## Linear (Projection) Layer + Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "6c6ea6dd",
      "metadata": {
        "id": "6c6ea6dd"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return torch.log_softmax(self.proj(x), dim = -1) #log softmax for numerical stability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919c89d8",
      "metadata": {
        "id": "919c89d8"
      },
      "source": [
        "## Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "952b4103",
      "metadata": {
        "id": "952b4103"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "        super().__init__()\n",
        "        self.src_embed = src_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.encoder = encoder\n",
        "\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.decoder = decoder\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d2b9cb",
      "metadata": {
        "id": "20d2b9cb"
      },
      "source": [
        "## Combining it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "28db2947",
      "metadata": {
        "id": "28db2947"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 128, N: int = 2, h: int=2, dropout: float = 0.1, d_ff: int = 1024): # N=num_layers, h=num_heads\n",
        "    # Create embedding layer\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create encoder and decoder\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "062b4eba",
      "metadata": {
        "id": "062b4eba"
      },
      "source": [
        "# dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "4b78ea55",
      "metadata": {
        "id": "4b78ea55"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_src.token_to_id('[PAD]')], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        # print(\"length of enc_tokens:\", len(enc_input_tokens))\n",
        "\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "        # print(\"length of dec:\", len(dec_input_tokens))\n",
        "        # print(\"seq len:\", self.seq_len)\n",
        "\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        # print(\"enc pad:\", enc_num_padding_tokens)\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "        # print(\"dec pad:\", dec_num_padding_tokens)\n",
        "\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError('Sentence is too long.')\n",
        "\n",
        "        # Add SOS and EOS tokens and padding to the source text\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add SOS to decoder input\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add EOS to the label (what we expect as output from the decoder)\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        assert encoder_input.shape[0] == self.seq_len\n",
        "        assert decoder_input.shape[0] == self.seq_len\n",
        "        assert label.shape[0] == self.seq_len\n",
        "\n",
        "        return {\n",
        "            'encoder_input': encoder_input, # (seq_len,)\n",
        "            'decoder_input': decoder_input, # (seq_len,)\n",
        "            'encoder_mask' : (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            'decoder_mask' : (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, 1, seq_len) & (1, seq_len, seq_len)\n",
        "            'label': label, # (seq_len,)\n",
        "            'src_text': src_text,\n",
        "            'tgt_text': tgt_text\n",
        "        }\n",
        "\n",
        "def causal_mask(size: int):\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3629d163",
      "metadata": {
        "id": "3629d163"
      },
      "source": [
        "# train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f334e2",
      "metadata": {
        "id": "89f334e2"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "a7f5c21b",
      "metadata": {
        "id": "a7f5c21b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# from dataset import BilingualDataset, causal_mask\n",
        "# from model import build_transformer\n",
        "# from config import get_weights_file_path, get_config\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "  sos_idx = tokenizer_src.token_to_id('[SOS]')\n",
        "  eos_idx = tokenizer_src.token_to_id('[EOS]')\n",
        "\n",
        "  # precompute the encoder output and reuse it for every token we get from the decoder\n",
        "  encoder_output = model.encode(source, source_mask)\n",
        "  # initialize decoder input with the start of sentence token\n",
        "  decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "  while True:\n",
        "    if decoder_input.size(1) == max_len:\n",
        "      break\n",
        "\n",
        "    # build mask for target (decoder input)\n",
        "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "    # calculate the output\n",
        "    out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "    # get the next token\n",
        "    prob = model.project(out[:, -1])\n",
        "    # select the token with the max probability (greedy search)\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "    if next_word == eos_idx:\n",
        "      break\n",
        "  return decoder_input.squeeze(0)\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
        "  model.eval()\n",
        "  count = 0\n",
        "\n",
        "  # source_texts = []\n",
        "  # expected = []\n",
        "  # predicted = []\n",
        "\n",
        "  # Size of the control window (just use a default vaule)\n",
        "  console_width = 80\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in validation_ds:\n",
        "      count += 1\n",
        "      encoder_input = batch['encoder_input'].to(device)\n",
        "      encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "      assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "      model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "      source_text = batch['src_text'][0]\n",
        "      target_text = batch['tgt_text'][0]\n",
        "      model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "      # source_texts.append(source_text)\n",
        "      # expected.append(target_text)\n",
        "      # predicted.append(model_out_text)\n",
        "\n",
        "      # Print to the console\n",
        "      print_msg('-'*console_width)\n",
        "      print_msg(f'SOURCE: {source_text}')\n",
        "      print_msg(f'TARGET: {target_text}')\n",
        "      print_msg(f'PREDICTED: {model_out_text}')\n",
        "\n",
        "      if count == num_examples:\n",
        "        break\n",
        "\n",
        "    # if writer:\n",
        "    #   # TorchMetrics ChartErrorRate, BLEU, WordErrorRate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens = ['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency = 2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b46864a",
      "metadata": {
        "id": "0b46864a"
      },
      "source": [
        "## Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "96c1dcec",
      "metadata": {
        "id": "96c1dcec"
      },
      "outputs": [],
      "source": [
        "def get_ds(config):\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Train-validate split, keep 90% for training and 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model\n",
        "\n",
        "def train_model(config):\n",
        "    # Define the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device {device}')\n",
        "\n",
        "    Path(config['model_folder']).mkdir(parents= True, exist_ok= True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
        "        for batch in batch_iterator:\n",
        "            model.train()\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (batch, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (batch, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)   # (1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)   # (1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the transformer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (batch, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (batch, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (batch, seq_len, tgt_vocab_size)\n",
        "\n",
        "            label = batch['label'].to(device) # (batch, seq_len)\n",
        "\n",
        "            # (batch, seq_len, tgt_vocab_size) --> (batch * seq_len, tgt_vocab_size)\n",
        "            # (batch, seq_len) --> (batch * seq_len)\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # log the loss\n",
        "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n",
        "\n",
        "\n",
        "# ----- Uncomment this part to train the model -----\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     warnings.filterwarnings('ignore')\n",
        "#     config = get_config()\n",
        "#     train_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Only Code"
      ],
      "metadata": {
        "id": "vwIVdn9--vS1"
      },
      "id": "vwIVdn9--vS1"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from config import get_config, get_weights_file_path\n",
        "# from train import get_model, get_ds, run_validation\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\",device)\n",
        "config = get_config()\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "# Load the pretrained weights\n",
        "model_filename = get_weights_file_path(config, f\"09\")\n",
        "state = torch.load(model_filename)\n",
        "model.load_state_dict(state['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZy6KPXETbod",
        "outputId": "8a9ddd16-222e-473e-8dbd-01f9ade47c58"
      },
      "id": "lZy6KPXETbod",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGGuhpBmALGg",
        "outputId": "4258a013-409c-41cf-80e2-73604df8fe3c"
      },
      "id": "RGGuhpBmALGg",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: I unhappy?\n",
            "TARGET: Io infelice?\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: But I should think she may make a better match,' said Vronsky, and expanding his chest he again moved forward. 'However, I don't know him,' he added.\n",
            "TARGET: Io penso, del resto, che lei può aspirare a un partito migliore — disse Vronskij e, raddrizzando il busto, si mise di nuovo a camminare. — Del resto, non lo conosco — soggiunse. — Già, deve essere una situazione penosa.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: By-and-by I saw a great fowl, like a hawk, sitting upon a tree within shot; so, to let Friday understand a little what I would do, I called him to me again, pointed at the fowl, which was indeed a parrot, though I thought it had been a hawk; I say, pointing to the parrot, and to my gun, and to the ground under the parrot, to let him see I would make it fall, I made him understand that I would shoot and kill that bird; accordingly, I fired, and bade him look, and immediately he saw the parrot fall.\n",
            "TARGET: Allora chiamatomi nuovamente da presso Venerdì per dargli in qualche modo a capire che cosa volessi fare, presi la mira al supposto falco che si trovò poi essere un pappagallo: ciò non fa nulla. Volsi dunque nel tempo stesso l’attenzione di Venerdì sul mio schioppo, su l’uccello e sul terreno che gli stava sotto, perchè notasse il luogo ove io divisava che cadesse la preda, dopo sparai facendogli subito osservare come l’effetto avesse pienamente corrisposto alle mie predizioni.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: So, after a great deal of time lost in searching for a stone, I gave it over, and resolved to look out for a great block of hard wood, which I found, indeed, much easier; and getting one as big as I had strength to stir, I rounded it, and formed it on the outside with my axe and hatchet, and then with the help of fire and infinite labour, made a hollow place in it, as the Indians in Brazil make their canoes.\n",
            "TARGET: Pensai di volgermi piuttosto ad un grosso ceppo di legno ben duro, impresa che del certo mi presentava minori difficoltà. Procuratomi pertanto un ceppo tanto grosso quanto le mie forze mi permettevano di moverlo, lo ritondai esternamente con la mia accetta; indi con l’aiuto del fuoco, nè senza un’immensa fatica, lavorai uno scavo dentr’esso nel modo con che gl’Indiani del Brasile si fabbricano i loro canotti.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Returning tired and hungry from his sport, Levin so vividly anticipated the pies that on approaching his lodging he seemed to smell and taste them – just as Laska scented game – and he immediately ordered Philip to serve them.\n",
            "TARGET: Ritornando stanco ed affamato dalla caccia, Levin sognava in modo così preciso gli sfogliantini che, accostandosi all’alloggio, ne pregustava l’odore e il sapore in bocca, così come Laska fiutava la selvaggina, e ordinò subito a Filipp di portarglieli.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: I don't.\n",
            "TARGET: Io no.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: And then there are other chances in life far more thrilling and rapture-giving: _this_ is solid, an affair of the actual world, nothing ideal about it: all its associations are solid and sober, and its manifestations are the same.\n",
            "TARGET: Vi sono gioie più inebrianti. Un patrimonio è un bene solido terrestre, ma non ha nulla d'ideale, tutto ciò che a quello ha rapporto è calmo e la gioia che si prova non può manifestarsi entusiasticamente.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: We lit our pipes, and sat, looking out on the quiet night, and talked.\n",
            "TARGET: Accendemmo tutti e tre la pipa, e seduti, guardando la calma notte, conversammo.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Crossing the yard past the heap of snow by the lilac bush, he reached the shed.\n",
            "TARGET: Attraversando il cortile, vicino al mucchio di neve che era accanto alle serenelle, Levin raggiunse la stalla.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: I can't do anything, begin anything, change anything!\n",
            "TARGET: Ma io non posso scrivere ancora.\n",
            "PREDICTED: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "gfe7oapQAv1C"
      },
      "id": "gfe7oapQAv1C"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}